{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_FILE_URL = 'https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt'\n",
    "\n",
    "# Let's download the file and save it somewhere\n",
    "from requests import get\n",
    "with open('big.txt', 'wb') as big_f:\n",
    "    response = get(BIG_FILE_URL, )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        big_f.write(response.content)\n",
    "    else:\n",
    "        print(\"Unable to get the file: {}\".format(response.reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
    "# the overall pipeline for various well-known tokenization algorithm. \n",
    "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence, BertNormalizer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Then we enable lower-casing and unicode-normalization\n",
    "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
    "# executed in order.\n",
    "tokenizer.normalizer = Sequence([\n",
    "    BertNormalizer(),\n",
    "    Lowercase()\n",
    "])\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = ByteLevelDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trained vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
    "tokenizer.train(files=[\"big.txt\"], trainer=trainer)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['.\\\\vocab.json', '.\\\\merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# You will see the generated files in the output.\n",
    "tokenizer.model.save('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoded string: ['Ġthis', 'Ġis', 'Ġa', 'Ġsimple', 'Ġin', 'put', 'Ġto', 'Ġbe', 'Ġtoken', 'ized']\n",
      "Decoded string:  this is a simple input to be tokenized\n",
      "C:\\Users\\calvotello\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Let's tokenizer a simple input\n",
    "tokenizer.model = BPE('vocab.json', 'merges.txt')\n",
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d35c46f8d1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      "a predefined tokenizer.\n",
      "\n",
      "Args:\n",
      "    pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
      "        Can be either:\n",
      "\n",
      "        - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      "          Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
      "          user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      "        - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      "          using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      "          method, e.g., ``./my_model_directory/``.\n",
      "        - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      "          file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      "          ``./my_model_directory/vocab.txt``.\n",
      "    cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      "        Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      "        standard cache should not be used.\n",
      "    force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "        Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      "        exist.\n",
      "    resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "        Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      "        exists.\n",
      "    proxies (:obj:`Dict[str, str], `optional`):\n",
      "        A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "    use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      "        The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      "        generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      "    revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "        git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      "        identifier allowed by git.\n",
      "    subfolder (:obj:`str`, `optional`):\n",
      "        In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      "        facebook/rag-token-base), specify it here.\n",
      "    inputs (additional positional arguments, `optional`):\n",
      "        Will be passed along to the Tokenizer ``__init__`` method.\n",
      "    kwargs (additional keyword arguments, `optional`):\n",
      "        Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      "        ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      "        ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      "\n",
      ".. note::\n",
      "\n",
      "    Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      "\n",
      "Examples::\n",
      "\n",
      "    # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      "    # Download vocabulary from huggingface.co and cache.\n",
      "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "\n",
      "    # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      "    tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      "\n",
      "    # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      "    tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      "\n",
      "    # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      "    tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      "\n",
      "    # You can link tokens to special vocabulary when instantiating\n",
      "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      "    # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      "    # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      "    assert tokenizer.unk_token == '<unk>'\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\calvotello\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\n",
      "\u001b[1;31mType:\u001b[0m      method\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "BertTokenizer.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tz = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Let's learn deep learning!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\calvotello\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Encode the sentence\n",
    "encoded = tz.encode_plus(\n",
    "    text=sent,  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = 64,  # maximum length of a sentence\n",
    "    pad_to_max_length=True,  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'np',  # ask the function to return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Get the input IDs and attention mask in tensor format\n",
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['no', '##e', 'le', 'di', '##jo', 'a', 'di', '##os']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "tz.tokenize(\"Noé le dijo a Dios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 101, 2421,  112,  188, 3858, 1996, 3776,  106,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "960f4d658457e0296618a96ff118bd2cbd6b6fed4373572a098ed4ec5a9be4c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}